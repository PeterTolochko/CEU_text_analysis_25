---
title: "Unsupervised Methods"
author: ""
date: ""
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Preparation

### Required Packages

We first need to install the packages required for further analysis.

```{r, echo=FALSE}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
```

```{r, message=FALSE, results='hide'}

# install.packages("tm")                            
# install.packages("tidyverse")                     
# install.packages("ggthemes")                      
# install.packages("ggrepel")
# install.packages("cowplot")
# install.packages("quanteda")
# install.packages("quanteda.textmodels")



# install.packages(quanteda.textplots)
# install.packages("gtools")
# install.packages("sotu")
# install.packages("stm")
# install.packages(c("Rtsne", "rsvd", "geometry", "purrr"))


```

Note: you only need to install the packages once.

We then need load the packages in our environment:

```{r, message=FALSE, results='hide'}
library(tm)
library(tidyverse)
library(ggthemes)
library(ggrepel)
library(quanteda)
library(quanteda.textmodels)
library(gtools)
library(sotu)
library(stm)
library(purrr)
library(quanteda.textplots)

```

# K-Means Clustering
## Let's first generate some data!

We will use multinomial models to generate data. Generate texts about bananas and chocolate.

```{r}
set.seed(123)

# Priors

theta_1 <- rdirichlet(1, c(1, 6))
theta_2 <- rdirichlet(1, c(6, 1))

vocabulary <- c("banana", "chocolate")

w <- 200 # numbers of words to generate
n <- 100

generate_text <- function(w, theta) {
  sample(vocabulary, rbinom(1, w, .5), replace = TRUE, prob = theta) %>%
  paste(collapse = " ")
}


text_type_1 <- replicate(n, generate_text(w, theta_1))

text_type_2 <- replicate(n, generate_text(w, theta_2))


all_texts <- c(text_type_1, text_type_2)



dtm <- DocumentTermMatrix(all_texts) %>% as.matrix()
dtm


dtm %>% as_tibble() %>%
  mutate(class = c(rep(1, n), rep(2, n))) %>%
  ggplot() +
  geom_point(aes(banana, chocolate, color = factor(class))) +
  theme_tufte()


```


# Now let's try clustering them!
It's very easy. We're using `kmeans` function from base `r`.

```{r}
clustering.kmeans <- kmeans(dtm, 2)
clustering.kmeans




```

Let's look at the cluster assignment:

```{r}

cluster <- clustering.kmeans$cluster
centroids <- clustering.kmeans$centers

cluster

centroids

```

And assign to our data:

```{r}
dtm %>% as_tibble() %>%
  mutate(cluster = cluster) %>%
  ggplot() +
  geom_point(aes(banana, chocolate, color = factor(cluster))) +
  geom_point(aes(banana, chocolate), data = as_tibble(centroids),
             size = 6, shape = 10) +
  theme_tufte()
```





Ok, let's add some more data!

```{r}

text_type_1 <- replicate(n, generate_text(w, rdirichlet(1, c(2, 8))))
text_type_2 <- replicate(n, generate_text(w, rdirichlet(1, c(8, 2))))



text_type_3 <- replicate(n, generate_text(w, rdirichlet(1, c(1, 1))))




all_texts <- c(text_type_1, text_type_2, text_type_3)
dtm <- DocumentTermMatrix(all_texts) %>% as.matrix()

dtm %>% as_tibble() %>%
  mutate(class = c(rep(1, n), rep(2, n), rep(3, n))) %>%
  ggplot() +
  geom_point(aes(banana, chocolate)) +
  theme_tufte()


```

A bit more interesting!

```{r}
clustering.kmeans <- kmeans(dtm, 2)
clustering.kmeans

cluster <- clustering.kmeans$cluster
centroids <- clustering.kmeans$centers


```

```{r}
dtm %>% as_tibble() %>%
  mutate(cluster = cluster) %>%
  ggplot() +
  geom_point(aes(banana, chocolate, color = factor(cluster))) +
  geom_point(aes(banana, chocolate), data = as_tibble(centroids),
             size = 6, shape = 10) +
  theme_tufte()
```


```{r}

set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(dtm, k, nstart = 10)$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")




```



# Topic Modeling

We will be using the `stm` package and `sotu` data.

First, let's load up the data:

```{r}

sotu <- sotu_text %>% as_tibble()
sotu_metadata <- sotu_meta %>% as_tibble()




sotu <- bind_cols(sotu, sotu_metadata)

summary(sotu)

```


Let's inspect a text:

```{r}
sotu$value[1]

sotu$value[220]

```

Also, the timeframe in the dataset is quite large (1790 to 2020). We might want to trim it, since the language might have changed significantly in this time.

```{r}
sotu <- sotu %>% 
  filter(year >= 1901)
```


Ok, we need to do basic preprocessing. We can do that immediately within the infrastructure of the `stm` package, with the `textProcessor` function.

This will take a few seconds...

```{r}
sotu_data <- textProcessor(
  sotu$value,
  metadata = sotu
)
```



Ok, now we have the processed object.

```{r}
sotu_data
```
Next, we need to prepare the documents for topic modeling. This is done with the `prepDocuments` function
```{r}
sotu_data_prepd <- prepDocuments(sotu_data$documents,
                                 sotu_data$vocab,
                                 sotu_data$meta)
```


Now we're ready for some modelling!
This will also take some time.

```{r, results='hide'}
sotu_topic_10 <- stm(documents = sotu_data_prepd$documents,
                     sotu_data_prepd$vocab,
                     K = 10,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral")
```

When the model is done running, we can inspect the output. Different possible ways to do so:

```{r}
labelTopics(sotu_topic_10)

```

Notice the word `will`, `nation`, `year`, `american`, `america`,  etc. appear in every topic. This is not a great topic model.

First, we might want to remove these terms.


```{r}
sotu_data <- textProcessor(
  sotu$value,
  metadata = sotu,
  customstopwords = c("will",
                      "national",
                      "nation",
                      "year",
                      "america",
                      "american",
                      "government",
                      "govern",
                      "can",
                      "must")
)

sotu_data_prepd <- prepDocuments(sotu_data$documents,
                                 sotu_data$vocab,
                                 sotu_data$meta)

sotu_topic_15 <- stm(documents = sotu_data_prepd$documents,
                     sotu_data_prepd$vocab,
                     K = 15,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral")

labelTopics(sotu_topic_15)
plot(sotu_topic_15, n = 10)
```


A little bit better, but still not amazing. Maybe the problem is in the K? How many topics to select? ***NOBODY KNOWS***.

But there are several ways to get a better idea:

### T-SNE initialization

```{r}
sotu_topic_0 <- stm(documents = sotu_data_prepd$documents,
                     sotu_data_prepd$vocab,
                     K = 0,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral")
```

WOAH, this method recommends 92 topics!!! But this is a greed method: essentially, the best 'fit' for a topic model is when every document is assigned a separate topic. But kind of defeats the purpose.

# Many Models

Second method is estimating several topic models and then comparing "quality statistics", and figuring out which one is better. Let's estimate K = 20, K = 40 and K = 60.

This takes a very long time! (I already ran it)


```{r, print=FALSE}
# many_models <- manyTopics(documents = sotu_data_prepd$documents,
#                      sotu_data_prepd$vocab,
#                      K = c(20, 40, 60),
#                      max.em.its = 75,
#                      data = sotu_data_prepd$meta,
#                      init.type = "Spectral")

load("many_models.RData")
```


```{r}
sapply(many_models$exclusivity, mean)
sapply(many_models$semcoh, mean)

```


# Exclusivity

How unique the top words of a topic are relative to other topics.

$excl(w, k) = \frac{\phi_{k, w}}{\sum_{k'}\phi_{k'w}} $


```{r}

# ----- Exclusivity Calculation -----
# Define word probabilities for each topic:
topic1_probs <- c(w1 = 0.50, w2 = 0.30, w3 = 0.20, w4 = 0.00)
topic2_probs <- c(w1 = 0.00, w2 = 0.40, w3 = 0.40, w4 = 0.20)

# Create a data frame of probabilities for words in each topic:
prob_df <- tibble(
  word = c("w1", "w2", "w3", "w4"),
  topic1 = topic1_probs,
  topic2 = topic2_probs
)

# Calculate exclusivity for words present in each topic:
prob_df <- prob_df %>%
  mutate(
    sum_probs = topic1 + topic2,
    excl_topic1 = ifelse(topic1 > 0, topic1 / sum_probs, NA),
    excl_topic2 = ifelse(topic2 > 0, topic2 / sum_probs, NA)
  )

print(prob_df)
# Calculate average exclusivity for each topic based on top words:
# For Topic 1, consider its top words: w1, w2, w3
avg_excl_topic1 <- mean(prob_df$excl_topic1[1:3], na.rm = TRUE)
# For Topic 2, consider its top words: w2, w3, w4
avg_excl_topic2 <- mean(prob_df$excl_topic2[2:4], na.rm = TRUE)

cat("Average Exclusivity for Topic 1:", round(avg_excl_topic1, 4), "\n")
cat("Average Exclusivity for Topic 2:", round(avg_excl_topic2, 4), "\n")
```


# Semantic coherence

How often the top words of a topic co-occur in documents.

$C(k) = \sum^M_{m=2}\sum^{m-1}_{l=1}log\frac{D(w_m, w_l) + 1}{D(w_l)} $

```{r}
# ----- Semantic Coherence Calculation -----
# Synthetic Document-Term Information:
# Define document frequencies for each word:
doc_freq <- tibble(
  word = c("w1", "w2", "w3", "w4"),
  freq = c(3, 4, 3, 2)  # D(w1)=3, D(w2)=4, D(w3)=3, D(w4)=2
)

# For Topic 1 (words: w1, w2, w3), define co-occurrence counts:
# D(w1, w2) = 2, D(w1, w3) = 1, D(w2, w3) = 3
co_occ_topic1 <- tibble(
  pair = c("w2_w1", "w3_w1", "w3_w2"),
  count = c(2, 1, 3)
)

# Calculate semantic coherence for Topic 1:
sc_w2_w1 <- log((co_occ_topic1$count[co_occ_topic1$pair == "w2_w1"] + 1) / 
                  doc_freq$freq[doc_freq$word == "w1"])
sc_w3_w1 <- log((co_occ_topic1$count[co_occ_topic1$pair == "w3_w1"] + 1) / 
                  doc_freq$freq[doc_freq$word == "w1"])
sc_w3_w2 <- log((co_occ_topic1$count[co_occ_topic1$pair == "w3_w2"] + 1) / 
                  doc_freq$freq[doc_freq$word == "w2"])

semantic_coherence_topic1 <- sc_w2_w1 + sc_w3_w1 + sc_w3_w2
cat("Semantic Coherence for Topic 1:", round(semantic_coherence_topic1, 4), "\n")
# Expected: 0 + log(2/3) + 0 ≈ -0.4055

# For Topic 2 (words: w2, w3, w4), define co-occurrence counts:
# D(w2, w3) = 3, D(w2, w4) = 1, D(w3, w4) = 1
sc_w3_w2_topic2 <- log((3 + 1) / doc_freq$freq[doc_freq$word == "w2"])  # log(4/4)=0
sc_w4_w2 <- log((1 + 1) / doc_freq$freq[doc_freq$word == "w2"])  # log(2/4)=log(0.5)
sc_w4_w3 <- log((1 + 1) / doc_freq$freq[doc_freq$word == "w3"])  # log(2/3)

semantic_coherence_topic2 <- sc_w3_w2_topic2 + sc_w4_w2 + sc_w4_w3
cat("Semantic Coherence for Topic 2:", round(semantic_coherence_topic2, 4), "\n")
# Expected: 0 + log(0.5) + log(2/3) ≈ -1.0986



```



Ok, let's choose the K=60 model.

```{r, print=FALSE}
sotu_topic_60 <- stm(documents = sotu_data_prepd$documents,
                    vocab = sotu_data_prepd$vocab,
                    K = 60,
                     prevalence =~ party + s(year),
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral",
                     verbose=FALSE
)
```

Let's see:

```{r}
labelTopics(sotu_topic_60)
plot(sotu_topic_60)
```


```{r}
cloud(sotu_topic_60, topic=13, scale=c(2,.25))

```


```{r}


sotu_topic_60_effects <- estimateEffect(1:20 ~ party + s(year), sotu_topic_60, meta = sotu_data_prepd$meta, uncertainty = "Global")
summary(sotu_topic_60_effects, topics=3)



plot(sotu_topic_60_effects, covariate = "party", topics = c(1:3), model = sotu_topic_60, method = "pointestimate",
     main = "Effect of Party on Topic Proportion", labeltype = "custom",
     custom.labels = c("Republican", "Democratic")
     )




```

# Bigrams for topic modeling

```{r}

sotu_tokens <- tokens(sotu$value,
                      remove_punct = TRUE,
                      remove_numbers = TRUE,
                      remove_symbols = TRUE)
sotu_tokens <- tokens_tolower(sotu_tokens)
sotu_tokens_nostop <- tokens_select(sotu_tokens, pattern = stopwords("english"), selection = "remove")
sotu_bigrams <- tokens_ngrams(sotu_tokens_nostop, n = 2)

sotu_bigrams <- tokens_select(sotu_bigrams, pattern = c("united_states"), selection = "remove")
# sotu_bigrams <- tokens_ngrams(sotu_tokens_nostop, n = 1:2)


dfmat_bigrams <- dfm(sotu_bigrams)

dfmat_bigrams <- dfm_trim(dfmat_bigrams, min_termfreq = 5)

sotu_bigrams_stm <- convert(dfmat_bigrams, to = "stm")

stm_bigrams <- stm(documents = sotu_bigrams_stm$documents,
                   vocab = sotu_bigrams_stm$vocab,
                   K = 10,  # choose the number of topics appropriately
                   max.em.its = 75,
                   data = sotu_bigrams_stm$meta,
                   init.type = "Spectral")

# Label the topics
labelTopics(stm_bigrams)
```                  
                   
# Content difference

```{r, print=FALSE}
sotu_topic_5 <- stm(documents = sotu_data_prepd$documents,
                    vocab = sotu_data_prepd$vocab,
                    K = 5,
                    prevalence = ~party,
                     content =~ party,
                     max.em.its = 75,
                     data = sotu_data_prepd$meta,
                     init.type = "Spectral",
                     verbose=TRUE
)

```


# Text scaling

From [https://burtmonroe.github.io/TextAsDataCourse/Tutorials/IntroductionToWordfish.nb.html](https://burtmonroe.github.io/TextAsDataCourse/Tutorials/IntroductionToWordfish.nb.html)


```{r}
# Irish budget speeches from 2010

toks_irish <- tokens(data_corpus_irishbudget2010, remove_punct = TRUE)
dfmat_irish <- dfm(toks_irish)
tmod_wf <- textmodel_wordfish(dfmat_irish, dir = c(2, 4))
summary(tmod_wf)



textplot_scale1d(tmod_wf)

textplot_scale1d(tmod_wf, groups = dfmat_irish$party)


textplot_scale1d(tmod_wf, margin = "features", 
                 highlighted = c("government", "global", "children", 
                                 "bank", "economy", "the", "citizenship",
                                 "productivity", "deficit"))

```

Topic models can also do unidimensional scaling!

```{r}
dfmat_irish_stm <- quanteda::convert(dfmat_irish, to = "stm")
names(dfmat_irish_stm)

irish_stmfit <- stm(documents = dfmat_irish_stm$documents, 
                     vocab = dfmat_irish_stm$vocab,
                     K = 2,
                     max.em.its = 75,
                     data = dfmat_irish_stm$meta,
                     init.type = "Spectral"
)


compare.df <- cbind(name=rownames(docvars(dfmat_irish)),wordfish = tmod_wf$theta, stm = irish_stmfit$theta[,2])
compare.df


```
